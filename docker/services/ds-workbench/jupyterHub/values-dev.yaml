# fullnameOverride and nameOverride distinguishes blank strings, null values,
# and non-blank strings. For more details, see the configuration reference.
fullnameOverride: ""
nameOverride:

# enabled is ignored by the jupyterhub chart itself, but a chart depending on
# the jupyterhub chart conditionally can make use this config option as the
# condition.
enabled:

# custom can contain anything you want to pass to the hub pod, as all passed
# Helm template values will be made available there.
custom: {}

# imagePullSecret is configuration to create a k8s Secret that Helm chart's pods
# can get credentials from to pull their images.
imagePullSecret:
  create: false
  automaticReferenceInjection: true
  registry:
  username:
  password:
  email:
# imagePullSecrets is configuration to reference the k8s Secret resources the
# Helm chart's pods can get credentials from to pull their images.
imagePullSecrets: []

# hub relates to the hub pod, responsible for running JupyterHub, its configured
# Authenticator class KubeSpawner, and its configured Proxy class
# ConfigurableHTTPProxy. KubeSpawner creates the user pods, and
# ConfigurableHTTPProxy speaks with the actual ConfigurableHTTPProxy server in
# the proxy pod.
hub:
  revisionHistoryLimit:
  config:
    JupyterHub:
      admin_access: true
      authenticator_class: dummy
  service:
    type: ClusterIP
    annotations: {}
    ports:
      nodePort:
    extraPorts: []
    loadBalancerIP:
  baseUrl: /jupyterhub/
  cookieSecret:
  initContainers: []
  nodeSelector:
    agentpool: llmpool
  tolerations: []
  concurrentSpawnLimit: 64
  consecutiveFailureLimit: 5
  activeServerLimit:
  deploymentStrategy:
    ## type: Recreate
    ## - sqlite-pvc backed hubs require the Recreate deployment strategy as a
    ##   typical PVC storage can only be bound to one pod at the time.
    ## - JupyterHub isn't designed to support being run in parallell. More work
    ##   needs to be done in JupyterHub itself for a fully highly available (HA)
    ##   deployment of JupyterHub on k8s is to be possible.
    type: Recreate
  db:
    type: sqlite-pvc
    upgrade:
    pvc:
      annotations: {}
      selector: {}
      accessModes:
        - ReadWriteOnce
      storage: 1Gi
      subPath:
      storageClassName:
    url:
    password:
  labels: {}
  annotations: {}
  command: []
  args: []    
  extraConfig:
    baseUrl: |
      c.JupyterHub.base_url = '/jupyterhub/'
    cull: |
      c.JupyterHub.services.append({
        'name': 'cull-idle',
        'admin': True,
        'command': [
            'python3',
            '/usr/local/bin/cull_idle_servers.py',
            '--url=http://127.0.0.1:8081/jupyterhub/hub/api',
            '--timeout=300',
            '--cull-every=120'
        ]
      })
    custom_templates: |
      c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")
    custom_authentication: |
      c.Authenticator.enable_auth_state = True
      from tornado import gen, web
      from jupyterhub.handlers import BaseHandler
      from jupyterhub.auth import Authenticator
      import jwt
      import requests

      class JWTAuthenticator(Authenticator):
    
          # auto_login = True

          def __init__(self, *args, **kwargs):
              super().__init__(*args, **kwargs)
              self.admin_users = {"1512"}

          @gen.coroutine
          def authenticate(self, handler, data):
              public_key = """-----BEGIN PUBLIC KEY-----
              MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzsRPseDrlr7E1kNdo7Pl
              6PdSRUr4opBxnA5CE2n7iXBYM2m14T0bKehSn+r7tGeIdYFzFfJvffqEoVKJ2en3
              GyB8RIy8rGSubM0D1S9Thb0Au3SPFKYYRL6qpGU0s3tRCyXhQL54h0IkN2K4cHXe
              FV0aSaMlimogC8DIsYqNxf9KqeFdvzghQftCv73eu0Urz5VbsfjL84bscTIYavg9
              nw4aKDM8kEZs68668nNxG0irmK6m+UT3qvHA1079VhriScR3p5BzmdejFzf3VV2/
              ir01XJNIgTqnRWjCFkYJogDvEOcvnBjktSVRvKaVPzfyDA0GZKbDYEKZ+NjDMiDw
              PQIDAQAB
              -----END PUBLIC KEY-----
              """
              token = data['password']    
              if not token:
                  return None
              
              try:
                  decoded_token = jwt.decode(token, public_key, algorithms=['RS256'])
                  print(decoded_token)
                  project_name = str(decoded_token.get("pid", ""))  # change
                  if not project_name:
                      return None
                                  
                  fileshare_api_endpoint = 'https://nuclios-fastapi-dev.azurewebsites.net/nuclios-product-api/projects/project/create-file-share'
                  headers = {'Authorization': f'Bearer {token}'}
                  data = {'folder_name' : project_name}
                  response = requests.post(fileshare_api_endpoint, json=data, headers=headers)

          
                  user_info = {
                      'name': project_name,
                      'auth_state': {
                          'project_name': project_name,
                          'cpu_guarantee': decoded_token.get("cpu_guarantee", "0.5"),
                          'cpu_limit': decoded_token.get("cpu_limit", "0.5"),
                          'mem_guarantee': decoded_token.get("mem_guarantee", "1G"),
                          'mem_limit': decoded_token.get("mem_limit", "1G"),
                          "nodepool": decoded_token.get("nodepool"),
                          "image": decoded_token.get("image"),

                      }
                  }
                  return user_info
              except jwt.ExpiredSignatureError:
                  handler.set_status(401)
                  handler.write("Token has expired")
                  return None
              except jwt.InvalidTokenError as e:
                  handler.set_status(401)
                  handler.write(f"Invalid token: {str(e)}")
                  return None
              except Exception as e:
                  handler.set_status(500)
                  handler.write(f"Unexpected error: {str(e)}")
                  print(str(e))
                  return None
          
          async def pre_spawn_start(self, user, spawner):
              namespace = "jupyterhub-dev"
              secret_name = "dsworkbench-secret"
              import base64
              import time
              from kubernetes_asyncio import client, config
              from kubernetes_asyncio.client.models import V1ObjectMeta, V1PersistentVolume, V1PersistentVolumeClaim

              auth_state = await user.get_auth_state()
              if auth_state:
                  async def check_and_create_pv(api_instance, project_name, storage_account_name, sharename):
                      pv_name = f"{project_name}-pv-{namespace}"
                      try:
                          pv = await api_instance.read_persistent_volume(name=pv_name)
                      except Exception:
                          pv = None

                      if not pv:
                          metadata = V1ObjectMeta(name=pv_name, annotations={"pv.kubernetes.io/provisioned-by": "file.csi.azure.com"})
                          pv_spec = V1PersistentVolume(
                              metadata=metadata,
                              spec={
                                  "capacity": {"storage": "1Gi"},
                                  "accessModes": ["ReadWriteMany"],
                                  "persistentVolumeReclaimPolicy": "Delete",
                                  "storageClassName": "azurefile-csi",
                                  "mountOptions": ["dir_mode=0777", "file_mode=0777", "uid=0", "gid=0", "mfsymlinks", "cache=strict", "nosharesock", "nobrl"],
                                  "csi": {
                                      "driver": "file.csi.azure.com",
                                      "volumeHandle": f"{storage_account_name}_{sharename}_{project_name}",
                                      "volumeAttributes": {
                                          "shareName": sharename,
                                          "folderName": project_name
                                      },
                                      "nodeStageSecretRef": {
                                          "name": secret_name,
                                          "namespace": namespace
                                      }
                                  }
                              }
                          )
                          pv = await api_instance.create_persistent_volume(body=pv_spec)
                      return pv

                  async def check_and_create_pvc(api_instance, project_name, pv_name, namespace):
                      pvc_name = f"{project_name}-pvc-{namespace}"
                      try:
                          pvc = await api_instance.read_namespaced_persistent_volume_claim(name=pvc_name, namespace=namespace)
                      except Exception:
                          pvc = None

                      if not pvc:
                          metadata = V1ObjectMeta(name=pvc_name)
                          pvc_spec = V1PersistentVolumeClaim(
                              metadata=metadata,
                              spec={
                                  "accessModes": ["ReadWriteMany"],
                                  "resources": {
                                      "requests": {
                                          "storage": "1Gi"
                                      }
                                  },
                                  "volumeName": pv_name,
                                  "storageClassName": "azurefile-csi"
                              }
                          )
                          pvc = await api_instance.create_namespaced_persistent_volume_claim(body=pvc_spec, namespace=namespace)
                      return pvc
                  
                  project_name = auth_state.get('project_name')
                  config.load_kube_config()
                  time.sleep(3)

                  api_instance = client.CoreV1Api()
                  namespace_secrets = await api_instance.read_namespaced_secret(secret_name, namespace)

                  storage_account_name_encoded_bytes = namespace_secrets.data.get("azurestorageaccountname")
                  storage_account_name = base64.b64decode(storage_account_name_encoded_bytes).decode('utf-8')

                  sharename_encoded_bytes = namespace_secrets.data.get("sharename")  # should come from secrets
                  sharename = base64.b64decode(sharename_encoded_bytes).decode('utf-8')


                  pv = await check_and_create_pv(api_instance, project_name, storage_account_name, sharename)
                  pvc = await check_and_create_pvc(api_instance, project_name, pv.metadata.name, namespace)
                  
                  spawner.environment = {"PROJECT_ID": auth_state.get('project_name'), "DSSTORE_BACKEND_URI": "https://nuclios-gen-ai-dev.mathco.com/dsstore"}
                  spawner.volumes.append({'name': f"{project_name}-fs-add-stor", 'persistentVolumeClaim': {'claimName': pvc.metadata.name}})
                  volume_location = f"/home/jovyan/{project_name}"
                  spawner.volume_mounts.append({'mountPath': volume_location, 'name': f"{project_name}-fs-add-stor", "subPath": "scripts"})
                  spawner.notebook_dir = volume_location

                  spawner.mem_guarantee = auth_state.get('mem_guarantee')
                  spawner.mem_limit = auth_state.get('mem_limit')
                  spawner.cpu_guarantee = float(auth_state.get('cpu_guarantee'))
                  spawner.cpu_limit = float(auth_state.get('cpu_limit'))
                  spawner.image = auth_state.get("image")
                  spawner.image_pull_policy = "Always"
                  if auth_state.get('nodepool'):
                      spawner.tolerations.append({"key": auth_state.get('nodepool'), "operator": "Equal", "value": "true", "effect": "NoSchedule"})
                      spawner.node_selector = {"compute": auth_state.get('nodepool')}

      c.JupyterHub.authenticator_class = JWTAuthenticator

      c.JupyterHub.tornado_settings = {
          'xsrf_cookies': False,
          'cookie_options': {"SameSite": "None", "Secure": True},
          'headers': {
              'Content-Security-Policy': "frame-ancestors 'self' http://localhost:3001 https://nuclios-ui-dev.mathco.com",
              'Access-Control-Allow-Origin': 'http://localhost:3001 https://nuclios-ui-dev.mathco.com',
              'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
              'Access-Control-Allow-Headers': 'Content-Type, Authorization',
              'Access-Control-Allow-Credentials': 'true',
          }
      }
      c.Spawner.args = ['--NotebookApp.tornado_settings={"headers":{"Content-Security-Policy": "frame-ancestors * self https://nuclios-ui-dev.mathco.com http://localhost:3001"}}']
      c.JupyterHub.redirect_to_server = True


  extraFiles: {}
  extraEnv:
    DSWORKBENCH_HOST: nuclios-gen-ai-dev.mathco.com
    clusterRoleName: pv-creator-dev
    clusterRoleBindingName: pv-creator-binding-dev
  extraContainers: []
  extraVolumes:
    - name: hub-templates
      configMap:
        name: hub-templates
  extraVolumeMounts:
    - name: hub-templates
      mountPath: /etc/jupyterhub/templates
  image:
    name: quay.io/jupyterhub/k8s-hub
    tag: "3.3.7"
    pullPolicy:
    pullSecrets: []
  resources: {}
  podSecurityContext:
    fsGroup: 1000
  containerSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    allowPrivilegeEscalation: false
  lifecycle: {}
  loadRoles: {}
  services: {}
  pdb:
    enabled: false
    maxUnavailable:
    minAvailable: 1
  networkPolicy:
    enabled: true
    ingress: []
    egress: []
    egressAllowRules:
      cloudMetadataServer: true
      dnsPortsCloudMetadataServer: true
      dnsPortsKubeSystemNamespace: true
      dnsPortsPrivateIPs: true
      nonPrivateIPs: true
      privateIPs: true
    interNamespaceAccessLabels: ignore
    allowedIngressPorts: []
  allowNamedServers: false
  namedServerLimitPerUser:
  authenticatePrometheus:
  redirectToServer:
  shutdownOnLogout:
  templatePaths: []
  templateVars: {}
  livenessProbe:
    # The livenessProbe's aim to give JupyterHub sufficient time to startup but
    # be able to restart if it becomes unresponsive for ~5 min.
    enabled: true
    initialDelaySeconds: 300
    periodSeconds: 10
    failureThreshold: 30
    timeoutSeconds: 3
  readinessProbe:
    # The readinessProbe's aim is to provide a successful startup indication,
    # but following that never become unready before its livenessProbe fail and
    # restarts it if needed. To become unready following startup serves no
    # purpose as there are no other pod to fallback to in our non-HA deployment.
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 2
    failureThreshold: 1000
    timeoutSeconds: 1
  existingSecret:
  serviceAccount:
    create: true
    name:
    annotations: {}
  extraPodSpec: {}

rbac:
  create: true

# proxy relates to the proxy pod, the proxy-public service, and the autohttps
# pod and proxy-http service.
proxy:
  secretToken:
  annotations: {}
  deploymentStrategy:
    ## type: Recreate
    ## - JupyterHub's interaction with the CHP proxy becomes a lot more robust
    ##   with this configuration. To understand this, consider that JupyterHub
    ##   during startup will interact a lot with the k8s service to reach a
    ##   ready proxy pod. If the hub pod during a helm upgrade is restarting
    ##   directly while the proxy pod is making a rolling upgrade, the hub pod
    ##   could end up running a sequence of interactions with the old proxy pod
    ##   and finishing up the sequence of interactions with the new proxy pod.
    ##   As CHP proxy pods carry individual state this is very error prone. One
    ##   outcome when not using Recreate as a strategy has been that user pods
    ##   have been deleted by the hub pod because it considered them unreachable
    ##   as it only configured the old proxy pod but not the new before trying
    ##   to reach them.
    type: Recreate
    ## rollingUpdate:
    ## - WARNING:
    ##   This is required to be set explicitly blank! Without it being
    ##   explicitly blank, k8s will let eventual old values under rollingUpdate
    ##   remain and then the Deployment becomes invalid and a helm upgrade would
    ##   fail with an error like this:
    ##
    ##     UPGRADE FAILED
    ##     Error: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
    ##     Error: UPGRADE FAILED: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
    rollingUpdate:
  # service relates to the proxy-public service
  service:
    type: LoadBalancer
    labels: {}
    annotations: {}
    nodePorts:
      http:
      https:
    disableHttpPort: false
    extraPorts: []
    loadBalancerIP:
    loadBalancerSourceRanges: []
  # chp relates to the proxy pod, which is responsible for routing traffic based
  # on dynamic configuration sent from JupyterHub to CHP's REST API.
  chp:
    revisionHistoryLimit:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: quay.io/jupyterhub/configurable-http-proxy
      # tag is automatically bumped to new patch versions by the
      # watch-dependencies.yaml workflow.
      #
      tag: "4.6.1" # https://github.com/jupyterhub/configurable-http-proxy/tags
      pullPolicy:
      pullSecrets: []
    extraCommandLineFlags: []
    livenessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 10
      failureThreshold: 30
      timeoutSeconds: 3
    readinessProbe:
      enabled: true
      initialDelaySeconds: 0
      periodSeconds: 2
      failureThreshold: 1000
      timeoutSeconds: 1
    resources: {}
    defaultTarget:
    errorTarget:
    extraEnv: {}
    nodeSelector: 
      agentpool: llmpool
    tolerations: []
    networkPolicy:
      enabled: true
      ingress: []
      egress: []
      egressAllowRules:
        cloudMetadataServer: true
        dnsPortsCloudMetadataServer: true
        dnsPortsKubeSystemNamespace: true
        dnsPortsPrivateIPs: true
        nonPrivateIPs: true
        privateIPs: true
      interNamespaceAccessLabels: ignore
      allowedIngressPorts: [http, https]
    pdb:
      enabled: false
      maxUnavailable:
      minAvailable: 1
    extraPodSpec: {}
  # traefik relates to the autohttps pod, which is responsible for TLS
  # termination when proxy.https.type=letsencrypt.
  traefik:
    revisionHistoryLimit:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: traefik
      # tag is automatically bumped to new patch versions by the
      # watch-dependencies.yaml workflow.
      #
      tag: "v2.11.0" # ref: https://hub.docker.com/_/traefik?tab=tags
      pullPolicy:
      pullSecrets: []
    hsts:
      includeSubdomains: false
      preload: false
      maxAge: 15724800 # About 6 months
    resources: {}
    labels: {}
    extraInitContainers: []
    extraEnv: {}
    extraVolumes: []
    extraVolumeMounts: []
    extraStaticConfig: {}
    extraDynamicConfig: {}
    nodeSelector: {}
    tolerations: []
    extraPorts: []
    networkPolicy:
      enabled: true
      ingress: []
      egress: []
      egressAllowRules:
        cloudMetadataServer: true
        dnsPortsCloudMetadataServer: true
        dnsPortsKubeSystemNamespace: true
        dnsPortsPrivateIPs: true
        nonPrivateIPs: true
        privateIPs: true
      interNamespaceAccessLabels: ignore
      allowedIngressPorts: [http, https]
    pdb:
      enabled: false
      maxUnavailable:
      minAvailable: 1
    serviceAccount:
      create: true
      name:
      annotations: {}
    extraPodSpec: {}
  secretSync:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: quay.io/jupyterhub/k8s-secret-sync
      tag: "3.3.7"
      pullPolicy:
      pullSecrets: []
    resources: {}
  labels: {}
  https:
    enabled: false
    type: letsencrypt
    #type: letsencrypt, manual, offload, secret
    letsencrypt:
      contactEmail:
      # Specify custom server here (https://acme-staging-v02.api.letsencrypt.org/directory) to hit staging LE
      acmeServer: https://acme-v02.api.letsencrypt.org/directory
    manual:
      key:
      cert:
    secret:
      name:
      key: tls.key
      crt: tls.crt
    hosts: []

# singleuser relates to the configuration of KubeSpawner which runs in the hub
# pod, and its spawning of user pods such as jupyter-myusername.
singleuser:
  podNameTemplate:
  extraTolerations: []
  nodeSelector: 
    agentpool: llmpool
  extraNodeAffinity:
    required: []
    preferred: []
  extraPodAffinity:
    required: []
    preferred: []
  extraPodAntiAffinity:
    required: []
    preferred: []
  networkTools:
    image:
      name: quay.io/jupyterhub/k8s-network-tools
      tag: "3.3.7"
      pullPolicy:
      pullSecrets: []
    resources: {}
  cloudMetadata:
    # block set to true will append a privileged initContainer using the
    # iptables to block the sensitive metadata server at the provided ip.
    blockWithIptables: true
    ip: 169.254.169.254
  networkPolicy:
    enabled: true
    ingress: []
    egress: []
    egressAllowRules:
      cloudMetadataServer: false
      dnsPortsCloudMetadataServer: true
      dnsPortsKubeSystemNamespace: true
      dnsPortsPrivateIPs: true
      nonPrivateIPs: true
      privateIPs: false
    interNamespaceAccessLabels: ignore
    allowedIngressPorts: []
  events: true
  extraAnnotations: {}
  extraLabels:
    hub.jupyter.org/network-access-hub: "true"
    release: helm-jupyterhub
  extraFiles: {}
  extraEnv: {}
  lifecycleHooks:
    postStart:
      exec:
        command:
          - /bin/sh
          - -c 
          - >
            pip install --index-url https://__token__:jsk5si2rykevgjibtb7hjgjlz54zvqk6zmiulkcmrr2ky42ycvma@pkgs.dev.azure.com/mathco-products/codex/_packaging/codex_feed/pypi/simple/ nuclios-dark-v2
            pip install --index-url https://__token__:jsk5si2rykevgjibtb7hjgjlz54zvqk6zmiulkcmrr2ky42ycvma@pkgs.dev.azure.com/mathco-products/codex/_packaging/codex_feed/pypi/simple/ nuclios-light-v2
  initContainers: []
  extraContainers: []
  allowPrivilegeEscalation: false
  uid: 1000
  fsGid: 100
  serviceAccountName:
  storage:
    type: none
    extraLabels: {}
    extraVolumes: []
    extraVolumeMounts: []
    static:
      pvcName:
      subPath: 
    capacity: 10Gi
    homeMountPath: /home/jovyan 
    dynamic:
      storageClass:
      pvcNameTemplate: claim-{username}{servername}
      volumeNameTemplate: volume-{username}{servername}
      storageAccessModes: [ReadWriteOnce]
  image:
    name: quay.io/jupyterhub/k8s-singleuser-sample
    tag: "3.3.7"
    pullPolicy:
    # pullSecrets: []
    pullSecrets:
    - name: secrets-mathcodex
  startTimeout: 300
  cpu:
    limit:
    guarantee:
  memory:
    limit:
    guarantee: 1G
  extraResource:
    limits: {}
    guarantees: {}
  cmd: jupyterhub-singleuser
  defaultUrl:
  extraPodConfig: {}
  profileList: []

# scheduling relates to the user-scheduler pods and user-placeholder pods.
scheduling:
  userScheduler:
    enabled: true
    revisionHistoryLimit:
    replicas: 2
    logLevel: 4
    # plugins are configured on the user-scheduler to make us score how we
    # schedule user pods in a way to help us schedule on the most busy node. By
    # doing this, we help scale down more effectively. It isn't obvious how to
    # enable/disable scoring plugins, and configure them, to accomplish this.
    #
    # plugins ref: https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins-1
    # migration ref: https://kubernetes.io/docs/reference/scheduling/config/#scheduler-configuration-migrations
    #
    plugins:
      score:
        # These scoring plugins are enabled by default according to
        # https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins
        # 2022-02-22.
        #
        # Enabled with high priority:
        # - NodeAffinity
        # - InterPodAffinity
        # - NodeResourcesFit
        # - ImageLocality
        # Remains enabled with low default priority:
        # - TaintToleration
        # - PodTopologySpread
        # - VolumeBinding
        # Disabled for scoring:
        # - NodeResourcesBalancedAllocation
        #
        disabled:
          # We disable these plugins (with regards to scoring) to not interfere
          # or complicate our use of NodeResourcesFit.
          - name: NodeResourcesBalancedAllocation
          # Disable plugins to be allowed to enable them again with a different
          # weight and avoid an error.
          - name: NodeAffinity
          - name: InterPodAffinity
          - name: NodeResourcesFit
          - name: ImageLocality
        enabled:
          - name: NodeAffinity
            weight: 14631
          - name: InterPodAffinity
            weight: 1331
          - name: NodeResourcesFit
            weight: 121
          - name: ImageLocality
            weight: 11
    pluginConfig:
      # Here we declare that we should optimize pods to fit based on a
      # MostAllocated strategy instead of the default LeastAllocated.
      - name: NodeResourcesFit
        args:
          scoringStrategy:
            resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
            type: MostAllocated
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      # IMPORTANT: Bumping the minor version of this binary should go hand in
      #            hand with an inspection of the user-scheduelrs RBAC resources
      #            that we have forked in
      #            templates/scheduling/user-scheduler/rbac.yaml.
      #
      #            Debugging advice:
      #
      #            - Is configuration of kube-scheduler broken in
      #              templates/scheduling/user-scheduler/configmap.yaml?
      #
      #            - Is the kube-scheduler binary's compatibility to work
      #              against a k8s api-server that is too new or too old?
      #
      #            - You can update the GitHub workflow that runs tests to
      #              include "deploy/user-scheduler" in the k8s namespace report
      #              and reduce the user-scheduler deployments replicas to 1 in
      #              dev-config.yaml to get relevant logs from the user-scheduler
      #              pods. Inspect the "Kubernetes namespace report" action!
      #
      #            - Typical failures are that kube-scheduler fails to search for
      #              resources via its "informers", and won't start trying to
      #              schedule pods before they succeed which may require
      #              additional RBAC permissions or that the k8s api-server is
      #              aware of the resources.
      #
      #            - If "successfully acquired lease" can be seen in the logs, it
      #              is a good sign kube-scheduler is ready to schedule pods.
      #
      name: registry.k8s.io/kube-scheduler
      # tag is automatically bumped to new patch versions by the
      # watch-dependencies.yaml workflow. The minor version is pinned in the
      # workflow, and should be updated there if a minor version bump is done
      # here. We aim to stay around 1 minor version behind the latest k8s
      # version.
      #
      tag: "v1.26.15" # ref: https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG
      pullPolicy:
      pullSecrets: []
    nodeSelector:
      agentpool: llmpool
    tolerations: []
    labels: {}
    annotations: {}
    pdb:
      enabled: true
      maxUnavailable: 1
      minAvailable:
    resources: {}
    serviceAccount:
      create: true
      name:
      annotations: {}
    extraPodSpec: {}
  podPriority:
    enabled: false
    globalDefault: false
    defaultPriority: 0
    imagePullerPriority: -5
    userPlaceholderPriority: -10
  userPlaceholder:
    enabled: true
    image:
      name: registry.k8s.io/pause
      # tag is automatically bumped to new patch versions by the
      # watch-dependencies.yaml workflow.
      #
      # If you update this, also update prePuller.pause.image.tag
      #
      tag: "3.9"
      pullPolicy:
      pullSecrets: []
    revisionHistoryLimit:
    replicas: 0
    labels: {}
    annotations: {}
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    resources: {}
  corePods:
    tolerations:
      - key: hub.jupyter.org/dedicated
        operator: Equal
        value: core
        effect: NoSchedule
      - key: hub.jupyter.org_dedicated
        operator: Equal
        value: core
        effect: NoSchedule
    nodeAffinity:
      matchNodePurpose: prefer
  userPods:
    tolerations:
      - key: hub.jupyter.org/dedicated
        operator: Equal
        value: user
        effect: NoSchedule
      - key: hub.jupyter.org_dedicated
        operator: Equal
        value: user
        effect: NoSchedule
    nodeAffinity:
      matchNodePurpose: prefer

# prePuller relates to the hook|continuous-image-puller DaemonsSets
prePuller:
  revisionHistoryLimit:
  labels: {}
  annotations: {}
  resources: {}
  containerSecurityContext:
    runAsUser: 65534 # nobody user
    runAsGroup: 65534 # nobody group
    allowPrivilegeEscalation: false
  extraTolerations: []
  # hook relates to the hook-image-awaiter Job and hook-image-puller DaemonSet
  hook:
    enabled: true
    pullOnlyOnChanges: true
    # image and the configuration below relates to the hook-image-awaiter Job
    image:
      name: quay.io/jupyterhub/k8s-image-awaiter
      tag: "3.3.7"
      pullPolicy:
      pullSecrets: []
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    podSchedulingWaitDuration: 10
    nodeSelector:
      agentpool: llmpool
    tolerations: []
    resources: {}
    serviceAccount:
      create: true
      name:
      annotations: {}
  continuous:
    enabled: true
  pullProfileListImages: true
  extraImages: {}
  pause:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: registry.k8s.io/pause
      # tag is automatically bumped to new patch versions by the
      # watch-dependencies.yaml workflow.
      #
      # If you update this, also update scheduling.userPlaceholder.image.tag
      #
      tag: "3.9"
      pullPolicy:
      pullSecrets: []

ingress:
  enabled: false
  annotations: {}
  ingressClassName:
  hosts: []
  pathSuffix:
  pathType: Prefix
  tls: []

# cull relates to the jupyterhub-idle-culler service, responsible for evicting
# inactive singleuser pods.
#
# The configuration below, except for enabled, corresponds to command-line flags
# for jupyterhub-idle-culler as documented here:
# https://github.com/jupyterhub/jupyterhub-idle-culler#as-a-standalone-script
#
cull:
  enabled: true
  users: false # --cull-users
  adminUsers: true # --cull-admin-users
  removeNamedServers: false # --remove-named-servers
  timeout: 3600 # --timeout
  every: 600 # --cull-every
  concurrency: 10 # --concurrency
  maxAge: 0 # --max-age

debug:
  enabled: false

global:
  safeToShowValues: false

# dsworkbench:
#   host: nuclios-gen-ai-dev.mathco.com
